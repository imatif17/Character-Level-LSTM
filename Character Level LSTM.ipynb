{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM in PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load in our required resources for data loading and model creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see those same characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 63, 20, 41, 67, 21, 14, 27, 39, 70, 70, 70, 57, 20, 41, 41, 11,\n",
       "       27, 15, 20,  6, 34, 82, 34, 21, 47, 27, 20, 14, 21, 27, 20, 82, 82,\n",
       "       27, 20, 82, 34,  4, 21,  3, 27, 21, 76, 21, 14, 11, 27, 59, 29, 63,\n",
       "       20, 41, 41, 11, 27, 15, 20,  6, 34, 82, 11, 27, 34, 47, 27, 59, 29,\n",
       "       63, 20, 41, 41, 11, 27, 34, 29, 27, 34, 67, 47, 27, 45, 12, 29, 70,\n",
       "       12, 20, 11, 56, 70, 70, 36, 76, 21, 14, 11, 67, 63, 34, 29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making training mini-batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    arr = arr[:batch_size*n_batches]\n",
    "    \n",
    "    arr = arr.reshape((n_seqs,-1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[23 63 20 41 67 21 14 27 39 70]\n",
      " [27 20  6 27 29 45 67 27 81 45]\n",
      " [76 34 29 56 70 70 49 31 21 47]\n",
      " [29 27  8 59 14 34 29 81 27 63]\n",
      " [27 34 67 27 34 47 35 27 47 34]\n",
      " [27 48 67 27 12 20 47 70 45 29]\n",
      " [63 21 29 27 58 45  6 21 27 15]\n",
      " [ 3 27 30 59 67 27 29 45 12 27]\n",
      " [67 27 34 47 29 19 67 56 27 24]\n",
      " [27 47 20 34  8 27 67 45 27 63]]\n",
      "\n",
      "y\n",
      " [[63 20 41 67 21 14 27 39 70 70]\n",
      " [20  6 27 29 45 67 27 81 45 34]\n",
      " [34 29 56 70 70 49 31 21 47 35]\n",
      " [27  8 59 14 34 29 81 27 63 34]\n",
      " [34 67 27 34 47 35 27 47 34 14]\n",
      " [48 67 27 12 20 47 70 45 29 82]\n",
      " [21 29 27 58 45  6 21 27 15 45]\n",
      " [27 30 59 67 27 29 45 12 27 47]\n",
      " [27 34 47 29 19 67 56 27 24 63]\n",
      " [47 20 34  8 27 67 45 27 63 21]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Defining the network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars),self.n_hidden,self.n_layers, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(self.n_hidden,len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        x,(h,c) = self.lstm(x,hc)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the `predict`  function\n",
    "\n",
    "The output of our RNN is from a fully-connected layer and it outputs a **distribution of next-character scores**.\n",
    "\n",
    "To actually get the next character, we apply a softmax function, which gives us a *probability* distribution that we can then sample to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 256, num_layers=3, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = CharRNN(chars, n_hidden=256, n_layers=3)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.4843... Val Loss: 3.4631\n",
      "Epoch: 1/1... Step: 20... Loss: 3.3642... Val Loss: 3.3801\n",
      "Epoch: 1/1... Step: 30... Loss: 3.3459... Val Loss: 3.3475\n",
      "Epoch: 1/1... Step: 40... Loss: 3.2925... Val Loss: 3.3150\n",
      "Epoch: 1/1... Step: 50... Loss: 3.2927... Val Loss: 3.2734\n",
      "Epoch: 1/1... Step: 60... Loss: 3.1886... Val Loss: 3.1984\n",
      "Epoch: 1/1... Step: 70... Loss: 3.0765... Val Loss: 3.1283\n",
      "Epoch: 1/1... Step: 80... Loss: 3.0220... Val Loss: 3.0219\n",
      "Epoch: 1/1... Step: 90... Loss: 2.9619... Val Loss: 2.9448\n",
      "Epoch: 1/1... Step: 100... Loss: 2.8174... Val Loss: 2.8217\n",
      "Epoch: 1/1... Step: 110... Loss: 2.6873... Val Loss: 2.6984\n",
      "Epoch: 1/1... Step: 120... Loss: 2.5965... Val Loss: 2.6239\n",
      "Epoch: 1/1... Step: 130... Loss: 2.5657... Val Loss: 2.5708\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "train(net, encoded, epochs=1, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be using a different model trained on gpu\n",
    "# saving the model\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annad whin ghe ho tand of he an ta than sathit has andad int hime her tha the wor on ther sater on ha calte of the wartin to the wos anding hersin the the\n",
      "dalenton, the the that he hose seut in so oled thing, and hit tham so hin har tas ath ansd his the had to the sor on sane he sarent to te hindes hin an he that ha sorad he and the hans ofo ho thin the wor has hod th men he withe whe he the whing an thered ho the he ha dimoned, saten thas sere tho chat and ane tith ath the ho sasede son of tho he and to has of onte anthis hint asd touther, suuthe whes ad oun hithe hud to ha too tore shus hat thand asd at hers at onle sare the se ther, singing ant that he sint hess, tenthe to her othe he souderithe afet the tounes ant he wathis as ol the thes saud the he timt one whas, shad\n",
      "as the aride he the hered as ile orathe he she what on the and the sater an he se ole on oteringet the the whas hat to so ha seed the salting her here he ane thas thumt the the he sar tha seont ange atonedens thes tou has as ane the ald ing one thas han thand at hhe had the the thed and tho he ante ha sil ote ane to thes, andenthid, he tho counen so af te athos and wath sa thit on tha hor and hat ond the thes torthed the had aled hes sant ond torite ther the her and tham she whe shin seoterese sotin he\n",
      "aned sing thim san antinging, to had the hardere hut his soo chiles.\n",
      "\n",
      "\"\" her asdilig ont he the ther and he were toulling hat he\n",
      "she wat ha he want to ha hade the sithad ho her and that sore and onle the tand, he wim samentin one af tith seul he wot onet heranse at he whut anse the ant ales on he wime her shithat\n",
      "\n",
      "hesind sate wente tarete hed toundens ate the sead to ho ho tha santind af hos ane the serenthe hit sime thos whad\n",
      "she sansth simen hared and the wasersed tortithe the had sousend womitha he hadd\n",
      "ther wishe the thor the whin ans ther an toulen ande the to heris onse the herd angt hich shos hins han at on site ansthin ho has he hit hing asd on hus toun he thes tho he wasle he andde of in tor th\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over on the gpu\n",
    "with open('rnn_model.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f,map_location=torch.device('cpu'))\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said.\n",
      "\n",
      "\"I have not come in. It is it all that I'm ashamed to her. Won't it all is a\n",
      "pink,.\"\n",
      "\n",
      "\"Was. We've been so late...\"\n",
      "\n",
      "And he sat down to his wife's beauty. \"What are you thinking of the tin only\n",
      "and\n",
      "freshness. I shall be so late, a man and say that is the day all of his wife\n",
      "to make. I did not speak, but it appaner for me that it's between that\n",
      "it of many to me that it was an one another, and I don't know., Stiva\n",
      "said, I don't know why, the ministrous arouse. When that setting you\n",
      "think that things to arrange him for anything in them, and she were see\n",
      "that,\" said Vronsky, standing the same to this position.\n",
      "\n",
      "The sense of the priving came in a sense of her. \"In a position?\" though\n",
      "was, and the same weight still, as soon and his heads that the particular artray\n",
      "stairched her face.\n",
      "\n",
      "\"I am so solution?\" he remembered to him. \"What a strange is the standarry in this\n",
      "sense of the thing in the second seconds that to say\n",
      "anything and the court, that is she we arrived or impares in the\n",
      "course of that stirred on the salt of the choice and humardaurs, the passable\n",
      "of homing. That's the prince was at once about her, and shall make it in the\n",
      "same. I want to blame, but I don't want to see your arms. He can be.\"\n",
      "\n",
      "\"Oh, if it is a court, that in the corn of my house, that I've been.\n",
      "\n",
      "Alexey Alexandrovitch had brought my creaking in the party in one way, and I can an\n",
      "indelition and carcide minutes and marsh, there were the cases of the\n",
      "mealing of the man that had been mercy on hinow,\" said Vronsky, smiling,\n",
      "and her father's presence went in to be a court in the day, and she had\n",
      "been and so all that to be stronger, and that the same people saying a sentender, and\n",
      "an any one of sudden to the position to him; he went up to the province to be\n",
      "three than any conversation with the chestnul and tribed attention to\n",
      "the patches. She was all on the contraction. The mere women.\n",
      "\n",
      "The principal said such a smile. \"They were not better, and the measures, and\n",
      "the price was saying anything. He has b\n"
     ]
    }
   ],
   "source": [
    "# Change cuda to True if you are using GPU!\n",
    "print(sample(loaded, 2000, cuda=False, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
